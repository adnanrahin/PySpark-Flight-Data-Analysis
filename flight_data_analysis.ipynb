{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafd8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import faulthandler\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b18db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_writer_parquet(df: DataFrame, mode: str, path: str) -> None:\n",
    "    (df\n",
    "     .write\n",
    "     .mode(mode)\n",
    "     .parquet(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f458e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data_set_to_df(path: str, spark: SparkSession) -> DataFrame:\n",
    "    df = (spark\n",
    "          .read\n",
    "          .format(\"csv\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .load(path))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb5dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_the_flight_that_canceled(flightDF: DataFrame) -> DataFrame:\n",
    "    canceled_flight = (flightDF\n",
    "                       .select('*')\n",
    "                       .where('CANCELLED = 1'))\n",
    "\n",
    "    return canceled_flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc712f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_airlines_total_number_of_flights_cancelled(flightDF: DataFrame, airlineDF: DataFrame) -> DataFrame:\n",
    "    join_airline_flights_df = (\n",
    "        flightDF.join(airlineDF.withColumnRenamed('AIRLINE', 'AIRLINE_NAME'),\n",
    "                      flightDF['AIRLINE'] == airlineDF['IATA_CODE'],\n",
    "                      'inner')\n",
    "    )\n",
    "\n",
    "    all_cancelled_flights = (\n",
    "        join_airline_flights_df\n",
    "            .select('*')\n",
    "            .where('CANCELLED = 1')\n",
    "    )\n",
    "\n",
    "    airline_and_number_flights_cancelled = (\n",
    "        all_cancelled_flights\n",
    "            .groupby('AIRLINE_NAME')\n",
    "            .count()\n",
    "            .withColumnRenamed('count', 'TOTAL_NUMBER_FLIGHTS_CANCELLED')\n",
    "            .orderBy('TOTAL_NUMBER_FLIGHTS_CANCELLED')\n",
    "    ).collect()\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('AIRLINE_NAME', StringType(), True),\n",
    "            StructField('TOTAL_NUMBER_OF_FLIGHTS_CANCELLED', StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(data=airline_and_number_flights_cancelled, schema=schema)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa391e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_number_of_departure_flight_from_airport_to(flightDF: DataFrame, airportDF: DataFrame) -> DataFrame:\n",
    "    all_departure_flights = (\n",
    "        flightDF\n",
    "            .select('*')\n",
    "            .where('CANCELLED != 1')\n",
    "    )\n",
    "\n",
    "    join_flights_and_airports = (\n",
    "        all_departure_flights.join(airportDF,\n",
    "                                   all_departure_flights['ORIGIN_AIRPORT'] == airportDF['IATA_CODE'],\n",
    "                                   'inner')\n",
    "    )\n",
    "\n",
    "    total_departure_flights_from_each_airport = (\n",
    "        join_flights_and_airports\n",
    "            .groupby('AIRPORT')\n",
    "            .count()\n",
    "            .withColumnRenamed('count', 'TOTAL_NUMBER_DEPARTURE_FLIGHTS')\n",
    "            .orderBy('TOTAL_NUMBER_DEPARTURE_FLIGHTS')\n",
    "    ).persist(storageLevel=StorageLevel.MEMORY_AND_DISK).collect()\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('AIRPORT_NAME', StringType(), True),\n",
    "            StructField('TOTAL_NUMBER_DEPARTURE_FLIGHTS', StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(data=total_departure_flights_from_each_airport, schema=schema)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "063d5dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_flight_cancelled_airline(flightDF: DataFrame, airlineDF: DataFrame) -> DataFrame:\n",
    "    cancelled_flights = (\n",
    "        flightDF\n",
    "            .select('*')\n",
    "            .where('CANCELLED = 1')\n",
    "    )\n",
    "\n",
    "    join_flights_and_airline = (\n",
    "        cancelled_flights.join(airlineDF.withColumnRenamed('AIRLINE', 'AIRLINE_NAME')\n",
    "                               , flightDF['AIRLINE'] == airlineDF['IATA_CODE'], 'inner')\n",
    "    )\n",
    "\n",
    "    find_max_cancelled_airline = (\n",
    "        join_flights_and_airline.groupby('AIRLINE_NAME')\n",
    "            .count()\n",
    "            .withColumnRenamed('count', 'TOTAL_NUMBER_CANCELLED_FLIGHTS')\n",
    "            .orderBy(col('TOTAL_NUMBER_CANCELLED_FLIGHTS').desc())\n",
    "            .limit(1)\n",
    "    ).persist(storageLevel=StorageLevel.MEMORY_AND_DISK).collect()\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('AIRLINE_NAME', StringType(), True),\n",
    "            StructField('TOTAL_NUMBER_CANCELLED_FLIGHTS', StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(data=find_max_cancelled_airline, schema=schema)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feee520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_distance_flown_each_airline(flightDF: DataFrame, airlineDF: DataFrame) -> DataFrame:\n",
    "    cancelled_flights = flightDF.select('*').where('CANCELLED == 1')\n",
    "\n",
    "    join_flights_and_airline = (\n",
    "        cancelled_flights.join(airlineDF.withColumnRenamed('AIRLINE', 'AIRLINE_NAME')\n",
    "                               , flightDF['AIRLINE'] == airlineDF['IATA_CODE'], 'inner')\n",
    "    )\n",
    "\n",
    "    columns_name = join_flights_and_airline.columns\n",
    "\n",
    "    filter_col = list(filter(lambda x: x != 'AIRLINE_NAME' and x != 'DISTANCE', columns_name))\n",
    "\n",
    "    new_df = join_flights_and_airline.drop(*filter_col)\n",
    "\n",
    "    total_distance_flown = (\n",
    "        new_df\n",
    "            .groupby(col('AIRLINE_NAME'))\n",
    "            .agg(sum('DISTANCE').alias('TOTAL_DISTANCE'))\n",
    "            .orderBy('TOTAL_DISTANCE', ascending=False)\n",
    "    ).persist(storageLevel=StorageLevel.MEMORY_AND_DISK).collect()\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('AIRLINE_NAME', StringType(), True),\n",
    "            StructField('TOTAL_DISTANCE', StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(data=total_distance_flown, schema=schema)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13cfadcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_departure_delay_of_airliner(flightDF: DataFrame, airlineDF: DataFrame) -> DataFrame:\n",
    "    filter_flight_data = (\n",
    "        flightDF.filter(flightDF.DEPARTURE_DELAY is not None and flightDF.DEPARTURE_DELAY > 0)\n",
    "    )\n",
    "\n",
    "    join_flights_and_airline = (\n",
    "        filter_flight_data.join(airlineDF.withColumnRenamed('AIRLINE', 'AIRLINE_NAME')\n",
    "                                , flightDF['AIRLINE'] == airlineDF['IATA_CODE'], 'inner')\n",
    "    )\n",
    "\n",
    "    columns_name = join_flights_and_airline.columns\n",
    "\n",
    "    filter_col = list(filter(lambda x: x != 'AIRLINE_NAME' and x != 'DEPARTURE_DELAY', columns_name))\n",
    "\n",
    "    delayed_flights_df = join_flights_and_airline.drop(*filter_col)\n",
    "\n",
    "    airline_flight_count = (\n",
    "        delayed_flights_df\n",
    "            .groupby('AIRLINE_NAME')\n",
    "            .agg(sum('DEPARTURE_DELAY').alias('TOTAL_DELAY'),\n",
    "                 count('AIRLINE_NAME').alias('TOTAL_DELAYED_FLIGHTS'))\n",
    "    ).persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    result_df = (\n",
    "        airline_flight_count\n",
    "            .withColumn('AVERAGE', airline_flight_count.TOTAL_DELAY / airline_flight_count.TOTAL_DELAYED_FLIGHTS)\n",
    "            .orderBy('AVERAGE', ascending=False)\n",
    "    ).persist(storageLevel=StorageLevel.MEMORY_AND_DISK).collect()\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('AIRLINE_NAME', StringType(), True),\n",
    "            StructField('TOTAL_DELAY', StringType(), True),\n",
    "            StructField('TOTAL_DELAYED_FLIGHTS', StringType(), True),\n",
    "            StructField('AVERAGE', StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(data=result_df, schema=schema)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "646eefd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "conf = SparkConf().setAppName('FlightDataAnalysis').setMaster('spark://172.26.176.1:7077')\n",
    "\n",
    "conf.set('spark.executor.memory', '16G')\n",
    "conf.set('spark.driver.memory', '16G')\n",
    "conf.set(\"spark.sql.shuffle.partitions\", '200')\n",
    "#conf.set('spark.dynamicAllocation.enabled', 'true')\n",
    "#conf.set('spark.dynamicAllocation.minExecutors', '2')\n",
    "#conf.set('spark.dynamicAllocation.maxExecutors', '20')\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder.config(conf=conf)\n",
    "        # .master(\"local[*]\")\n",
    "        # .config(\"spark.executor.memory\", \"16G\")\n",
    "        # .config(\"spark.driver.memory\", \"16G\")\n",
    "        # .appName(\"FlightDataAnalysis\")\n",
    "         .getOrCreate())\n",
    "\n",
    "data_source_path = 'C:\\\\Users\\\\rahin\\\\source-code\\\\PycharmProjects\\\\PySpark-Filght-Data-Analysis\\\\2015_flights_data'\n",
    "\n",
    "flightDF = loading_data_set_to_df(path=data_source_path + '/flights.csv', spark=spark)\n",
    "airlineDF = loading_data_set_to_df(path=data_source_path + '/airlines.csv', spark=spark)\n",
    "airportDF = loading_data_set_to_df(path=data_source_path + '/airports.csv', spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0d0a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled_flight_df = find_all_the_flight_that_canceled(flightDF=flightDF)\n",
    "data_writer_parquet(cancelled_flight_df, 'overwrite',\n",
    "                            './transform_data/cancelled_flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be94d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flight_cancelled_by_airline_name = find_airlines_total_number_of_flights_cancelled(flightDF=flightDF,\n",
    "                                                                                                 airlineDF=airlineDF)\n",
    "data_writer_parquet(total_flight_cancelled_by_airline_name, 'overwrite',\n",
    "                            './transform_data/airline_total_flights_cancelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a333a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_departure_flights_from_each_airport = find_total_number_of_departure_flight_from_airport_to(\n",
    "            flightDF=flightDF, airportDF=airportDF)\n",
    "data_writer_parquet(total_departure_flights_from_each_airport, 'overwrite',\n",
    "                            './transform_data/total_number_departure_flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba867f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_departure_flights_from_each_airport = find_max_flight_cancelled_airline(\n",
    "            flightDF=flightDF, airlineDF=airlineDF)\n",
    "data_writer_parquet(total_departure_flights_from_each_airport, 'overwrite',\n",
    "                            './transform_data/most_cancelled_flights_airline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e5206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_distance_flown = find_total_distance_flown_each_airline(\n",
    "            flightDF=flightDF, airlineDF=airlineDF)\n",
    "data_writer_parquet(total_distance_flown, 'overwrite',\n",
    "                            './transform_data/total_distance_flown_each_airline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff1557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delayed_average = find_average_departure_delay_of_airliner(\n",
    "            flightDF=flightDF, airlineDF=airlineDF)\n",
    "data_writer_parquet(total_delayed_average, 'overwrite',\n",
    "                            './transform_data/average_departure_delay_of_airliner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cae996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark_env] *",
   "language": "python",
   "name": "conda-env-pyspark_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
